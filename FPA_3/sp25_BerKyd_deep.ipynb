{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1><center> ASDS 5303 Final Project Assignment #3 Dataset 1: Drug SMILES Strings and Classifications - Deep Network Analysis </center></h1>"
      ],
      "metadata": {
        "id": "iBjuncnlnQzx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Group Members\n",
        "### Henry Berrios #1001392315\n",
        "### LeMaur Kydd #1001767382"
      ],
      "metadata": {
        "id": "FiFKuEIJnziU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries (same as previous assignments, partially filled by google colab autofill)\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.io import wavfile\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import tensorflow as tf\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "iBcmkfnjoTyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **A. Create a Deep Neural Network**"
      ],
      "metadata": {
        "id": "0xh23XOToA9m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcwUMeCsksq2"
      },
      "outputs": [],
      "source": [
        "# using cuda if we can\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading pre processed data\n",
        "X_train = torch.load('/content/drive/MyDrive/X_train_tensor_d1.pt').to(device)\n",
        "X_test = torch.load('/content/drive/MyDrive/X_test_tensor_d1.pt').to(device)\n",
        "y_train = torch.load('/content/drive/MyDrive/y_train_tensor_d1.pt').to(device)\n",
        "y_test = torch.load('/content/drive/MyDrive/y_test_tensor_d1.pt').to(device)"
      ],
      "metadata": {
        "id": "zTZh3-IgpJB9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6213a3a6-2dc4-472b-d49d-d8eb63e52fd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-d0af1adbe808>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  X_train = torch.load('/content/drive/MyDrive/X_train_tensor_d1.pt').to(device)\n",
            "<ipython-input-3-d0af1adbe808>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  X_test = torch.load('/content/drive/MyDrive/X_test_tensor_d1.pt').to(device)\n",
            "<ipython-input-3-d0af1adbe808>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  y_train = torch.load('/content/drive/MyDrive/y_train_tensor_d1.pt').to(device)\n",
            "<ipython-input-3-d0af1adbe808>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  y_test = torch.load('/content/drive/MyDrive/y_test_tensor_d1.pt').to(device)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking shape of tensors\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1E4eDK8vCyM",
        "outputId": "0e34c7a8-f8c8-4968-fd10-7128ebb801e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5548, 399, 128])\n",
            "torch.Size([5548, 5])\n",
            "torch.Size([1387, 399, 128])\n",
            "torch.Size([1387, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a neural network (debugging assisted by OpenAI's ChatGPT)\n",
        "class DeepNetwork(nn.Module):\n",
        "  def __init__(self, input_size, hidden_layers, hidden_units, output_size):\n",
        "    super(DeepNetwork, self).__init__()\n",
        "    layers = []\n",
        "    layers.append(nn.Linear(input_size, hidden_units))\n",
        "    layers.append(nn.ReLU())\n",
        "\n",
        "    for _ in range(hidden_layers - 1):\n",
        "      layers.append(nn.Linear(hidden_units, hidden_units))\n",
        "      layers.append(nn.ReLU())\n",
        "\n",
        "    layers.append(nn.Linear(hidden_units, output_size))\n",
        "    self.model = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(x.size(0), -1) # flattens the output\n",
        "    return self.model(x)"
      ],
      "metadata": {
        "id": "SbnxFj0fqgy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing on small sample\n",
        "sample_inputs = X_train[:100].to(device)"
      ],
      "metadata": {
        "id": "Kiuta3CDub7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# passing through the model\n",
        "deep_test_model = DeepNetwork(input_size = sample_inputs.shape[1] * sample_inputs.shape[2], hidden_layers = 2, hidden_units = 128, output_size = y_train.shape[1]).to(device)\n",
        "\n",
        "sample_output = deep_test_model(sample_inputs)\n",
        "\n",
        "print(sample_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BqQjQEwwDsl",
        "outputId": "bc683ec3-176a-40bb-e248-44dc344be108"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for training errors (large portions of code are autofilled by google colab's gemini)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(deep_test_model.parameters(), lr=0.001)\n",
        "\n",
        "sample_train = y_train[:100].to(device)\n",
        "optimizer.zero_grad()\n",
        "outputs = deep_test_model(sample_inputs)\n",
        "loss = criterion(outputs, sample_train)\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4B85v-9xMf2",
        "outputId": "86f533bb-beee-4f49-bae2-1eba96759b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.662506103515625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training on small sample (large portions of code are autofilled by google colab's gemini)\n",
        "small_train_loader = DataLoader(TensorDataset(X_train[:10], y_train[:10]), batch_size=2, shuffle=True)\n",
        "\n",
        "for epoch in range(10):\n",
        "  deep_test_model.train()\n",
        "  epoch_loss = 0\n",
        "\n",
        "  for batch_x, batch_y in small_train_loader:\n",
        "    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = deep_test_model(batch_x)\n",
        "    loss = criterion(outputs, batch_y)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "  print(f\"Epoch {epoch+1}, Loss: {epoch_loss/len(small_train_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ic5oKgLAxoj_",
        "outputId": "6f9bc791-bccc-43f6-fa7f-9eb5e0fb0187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 6.954846954345703\n",
            "Epoch 2, Loss: 2.1673967123031614\n",
            "Epoch 3, Loss: 0.9131421744823456\n",
            "Epoch 4, Loss: 0.40645577274262906\n",
            "Epoch 5, Loss: 0.22919289506971835\n",
            "Epoch 6, Loss: 0.2770965125411749\n",
            "Epoch 7, Loss: 0.2771903851957177\n",
            "Epoch 8, Loss: 0.013115176698192954\n",
            "Epoch 9, Loss: 0.023283789050765336\n",
            "Epoch 10, Loss: 0.011135860963258892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **B. Optimize Network Depth and Width**"
      ],
      "metadata": {
        "id": "fvcYigbxoFJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizing network depth and width (large portions of code are autofilled by google colab's gemini)\n",
        "def train_deep_model(model, train_loader, test_loader, epochs = 20, lr = 0.001):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  train_losses = []\n",
        "  test_losses = []\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    for batch_x, batch_y in train_loader:\n",
        "      batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(batch_x)\n",
        "      loss = criterion(outputs, batch_y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      train_loss += loss.item()\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "      for batch_x, batch_y in test_loader:\n",
        "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "        outputs = model(batch_x)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    test_loss /= len(test_loader)\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss}, Test Loss: {test_loss}\")\n",
        "\n",
        "  return train_losses, test_losses"
      ],
      "metadata": {
        "id": "tLvEZ7hCoKhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing optimization\n",
        "batch_size = 256 # trying different batch sizes"
      ],
      "metadata": {
        "id": "VRJP19Ti2v00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating dataset to use for training and testing\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "# creating dataloaders for batch processing\n",
        "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
        "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)"
      ],
      "metadata": {
        "id": "wv3lwfxy4E8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# experimenting with depths and widths\n",
        "hidden_layers_list = [1, 2, 3]\n",
        "hidden_units_list = [32, 64, 128]"
      ],
      "metadata": {
        "id": "QOWvjTCC3Gxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing variables to track best model configuration\n",
        "best_loss = float('inf')\n",
        "best_config = None"
      ],
      "metadata": {
        "id": "qu8jxWVh3NYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing results for comparison\n",
        "results = []"
      ],
      "metadata": {
        "id": "ceC3UzdO4RP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# looping through different model configurations to find the best one (large portions of code are autofilled by google colab's gemini)\n",
        "for hidden_layers in hidden_layers_list:\n",
        "  for hidden_units in hidden_units_list:\n",
        "    print(f\"Training model with {hidden_layers} hidden layers and {hidden_units} hidden units\")\n",
        "    deep_model = DeepNetwork(input_size = X_train.shape[1] * X_train.shape[2], hidden_layers = hidden_layers, hidden_units = hidden_units, output_size = y_train.shape[1]).to(device)\n",
        "    train_losses, test_losses = train_deep_model(deep_model, train_loader, test_loader, epochs = 5, lr = 0.001)\n",
        "\n",
        "    final_test = test_losses[-1]\n",
        "    results.append((hidden_layers, hidden_units, final_test))\n",
        "\n",
        "    if final_test < best_loss:\n",
        "      best_loss = final_test\n",
        "      best_config = (hidden_layers, hidden_units)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMhk5WR34ix7",
        "outputId": "319ec35e-c90e-4547-ad75-39df4c761588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with 1 hidden layers and 32 hidden units\n",
            "Epoch 1, Train Loss: 2.5650275096125985, Test Loss: 1.6143889210440896\n",
            "Epoch 2, Train Loss: 1.6046539092886036, Test Loss: 1.5984236543828791\n",
            "Epoch 3, Train Loss: 1.5437473140913864, Test Loss: 1.4823730046098882\n",
            "Epoch 4, Train Loss: 1.4411429591562557, Test Loss: 1.456859534437006\n",
            "Epoch 5, Train Loss: 1.4074589356608775, Test Loss: 1.5018726587295532\n",
            "Training model with 1 hidden layers and 64 hidden units\n",
            "Epoch 1, Train Loss: 2.7267442229150354, Test Loss: 1.4953981041908264\n",
            "Epoch 2, Train Loss: 1.317820024216312, Test Loss: 1.529812373898246\n",
            "Epoch 3, Train Loss: 1.2036489063295825, Test Loss: 1.3158719593828374\n",
            "Epoch 4, Train Loss: 1.124148160561748, Test Loss: 1.3027807149020108\n",
            "Epoch 5, Train Loss: 1.1471550786632232, Test Loss: 1.3226164850321682\n",
            "Training model with 1 hidden layers and 128 hidden units\n",
            "Epoch 1, Train Loss: 3.536744223243889, Test Loss: 1.4291713562878696\n",
            "Epoch 2, Train Loss: 1.3208346065433545, Test Loss: 1.3843351656740361\n",
            "Epoch 3, Train Loss: 1.2483748361982148, Test Loss: 1.3442987311970105\n",
            "Epoch 4, Train Loss: 1.1432498172781933, Test Loss: 1.3455767198042436\n",
            "Epoch 5, Train Loss: 1.1501369106358494, Test Loss: 1.3250496766783975\n",
            "Training model with 2 hidden layers and 32 hidden units\n",
            "Epoch 1, Train Loss: 1.5973946335671962, Test Loss: 1.4358881224285474\n",
            "Epoch 2, Train Loss: 1.378842470289647, Test Loss: 1.442609879103574\n",
            "Epoch 3, Train Loss: 1.3008368070098175, Test Loss: 1.3540146567604758\n",
            "Epoch 4, Train Loss: 1.2109286469974736, Test Loss: 1.3226091211492366\n",
            "Epoch 5, Train Loss: 1.1303163835372048, Test Loss: 1.3444677808068015\n",
            "Training model with 2 hidden layers and 64 hidden units\n",
            "Epoch 1, Train Loss: 1.69819472713032, Test Loss: 1.3398318128152327\n",
            "Epoch 2, Train Loss: 1.274787752107642, Test Loss: 1.3255090442570774\n",
            "Epoch 3, Train Loss: 1.2225055208151367, Test Loss: 1.40275977416472\n",
            "Epoch 4, Train Loss: 1.175570601019366, Test Loss: 1.3345599120313472\n",
            "Epoch 5, Train Loss: 1.0949068213331288, Test Loss: 1.3355692408301614\n",
            "Training model with 2 hidden layers and 128 hidden units\n",
            "Epoch 1, Train Loss: 2.233009315085137, Test Loss: 1.478137807412581\n",
            "Epoch 2, Train Loss: 1.352136561240273, Test Loss: 1.4235725727948276\n",
            "Epoch 3, Train Loss: 1.2316460451860538, Test Loss: 1.2907867485826665\n",
            "Epoch 4, Train Loss: 1.1802452206611633, Test Loss: 1.485927858135917\n",
            "Epoch 5, Train Loss: 1.0847527562886818, Test Loss: 1.2815309437838467\n",
            "Training model with 3 hidden layers and 32 hidden units\n",
            "Epoch 1, Train Loss: 1.4978492547725808, Test Loss: 1.4260503487153486\n",
            "Epoch 2, Train Loss: 1.3851048398291927, Test Loss: 1.443272888660431\n",
            "Epoch 3, Train Loss: 1.2968662100276729, Test Loss: 1.3436623107303272\n",
            "Epoch 4, Train Loss: 1.2244498140510471, Test Loss: 1.4049386002800681\n",
            "Epoch 5, Train Loss: 1.1779849933481765, Test Loss: 1.3500034104694019\n",
            "Training model with 3 hidden layers and 64 hidden units\n",
            "Epoch 1, Train Loss: 1.5239671455032524, Test Loss: 1.4376903122121638\n",
            "Epoch 2, Train Loss: 1.3421321134457642, Test Loss: 1.3401431495493108\n",
            "Epoch 3, Train Loss: 1.24939251014556, Test Loss: 1.3680252703753384\n",
            "Epoch 4, Train Loss: 1.1675256518111832, Test Loss: 1.354446606202559\n",
            "Epoch 5, Train Loss: 1.088769822285093, Test Loss: 1.3175518404353748\n",
            "Training model with 3 hidden layers and 128 hidden units\n",
            "Epoch 1, Train Loss: 1.6318239466897373, Test Loss: 1.4131810448386453\n",
            "Epoch 2, Train Loss: 1.369946120799273, Test Loss: 1.3800166140903125\n",
            "Epoch 3, Train Loss: 1.2689989421559476, Test Loss: 1.3806999054822056\n",
            "Epoch 4, Train Loss: 1.183569054493959, Test Loss: 1.3009308034723455\n",
            "Epoch 5, Train Loss: 1.1241796256481915, Test Loss: 1.362702640620145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# printing results to find the simplest model\n",
        "best_loss = best_loss * 1.05\n",
        "\n",
        "# sort by the layers and units for the simplest model\n",
        "sort_results = sorted(results, key=lambda x: (x[0], x[1]))\n",
        "\n",
        "# finding the simplest model that is within the threshold\n",
        "for hidden_layers, hidden_units, test_loss in sort_results:\n",
        "  if test_loss <= best_loss:\n",
        "   simplest_config = (hidden_layers, hidden_units)\n",
        "   simplest_loss = test_loss\n",
        "   break\n",
        "\n",
        "# printing results\n",
        "print(f\"The simplest model has {simplest_config[0]} hidden layer(s) and {simplest_config[1]} hidden units\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtwfjROZ5N6K",
        "outputId": "ccd4be81-e07f-40f8-9d94-4df0a253879d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The simplest model has 1 hidden layer(s) and 64 hidden units\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# definining the function to evaluate the model\n",
        "def evaluate_model(model, test_loader):\n",
        "  model.eval()\n",
        "  y_true = []\n",
        "  y_pred = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_x, batch_y in test_loader:\n",
        "      batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "      outputs = model(batch_x)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "      y_true.extend(batch_y.argmax(dim=1).cpu().numpy())\n",
        "      y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "    # commpting metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # printing results\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "    return accuracy, precision, recall, f1, conf_matrix"
      ],
      "metadata": {
        "id": "vlhIlJ0KhP05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the best model\n",
        "best_model = DeepNetwork(input_size = X_train.shape[1] * X_train.shape[2], hidden_layers = best_config[0], hidden_units = best_config[1], output_size = y_train.shape[1]).to(device)"
      ],
      "metadata": {
        "id": "q1_Er_PAi53Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retraining the best model\n",
        "train_losses, test_losses = train_deep_model(best_model, train_loader, test_loader, epochs = 5, lr = 0.001)\n",
        "\n",
        "# evaluate the best model\n",
        "evaluate_model(best_model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VfShvXyjHZS",
        "outputId": "37fa7919-e8b0-4936-8b63-6e97215184db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 2.1833126723081215, Test Loss: 1.4851169911297886\n",
            "Epoch 2, Train Loss: 1.3230576665922142, Test Loss: 1.3261074748906223\n",
            "Epoch 3, Train Loss: 1.1715473292887897, Test Loss: 1.4478619748895818\n",
            "Epoch 4, Train Loss: 1.0964956345229313, Test Loss: 1.3739987178282305\n",
            "Epoch 5, Train Loss: 1.070602262842244, Test Loss: 1.2783523960547014\n",
            "Accuracy: 0.4939\n",
            "Precision: 0.5120\n",
            "Recall: 0.4939\n",
            "F1 Score: 0.4940\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.4938716654650324,\n",
              " 0.5120296223240731,\n",
              " 0.4938716654650324,\n",
              " 0.4939515456512623,\n",
              " array([[266,  52,  24,  38,  99],\n",
              "        [ 37,  91,  12,  26,  69],\n",
              "        [ 26,  13,  40,  29,  50],\n",
              "        [ 24,  24,  23, 106,  51],\n",
              "        [ 33,  28,  14,  30, 182]]))"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **C. Compare Results**"
      ],
      "metadata": {
        "id": "mkb-dhvnoMAh"
      }
    }
  ]
}